# Hyperparameter tuning, Batch Normalization, Programming Frameworks

### 1. If searching among a large number of hyperparameters, you should try values in a grid rather than random values, so that you can carry out the search more systematically and not rely on chance. True or False?
- False
---

### 2. Every hyperparameter, if set poorly, can have a huge negative impact on training, and so all hyperparameters are about equally important to tune well. True or False?
- False
> We've seen in lecture that some hyperparameters, such as the learning rate, are more critical than others.
---

### 3. During hyperparameter search, whether you try to babysit one model (“Panda” strategy) or train a lot of models in parallel (“Caviar”) is largely determined by:
- The amount of computational power you can access
---

### 4. If you think <img src="https://latex.codecogs.com/svg.image?\beta&space;" title="\beta " />(hyperparameter for momentum) is between 0.9 and 0.99, which of the following is the recommended way to sample a value for beta?
    r = np.random.rand()
    <img src="https://latex.codecogs.com/svg.image?\beta&space;=&space;1-&space;100^{-r-1}&space;" title="\beta = 1- 100^{-r-1} " />
---
### 5.

---
### 6.

---
### 7.

---
### 8.

---
### 9.

---
### 10.
